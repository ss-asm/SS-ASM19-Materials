---
title: "SSASM Networks"
output:
  html_document:
    number_sections: yes
    toc: yes
  slidy_presentation: default
  ioslides_presentation:
    smaller: yes
    widescreen: yes
  revealjs::revealjs_presentation:
    theme: league
---

```{r setup, include=FALSE}
#install.packages("foreign")
library(igraph)
library(lattice)
library(Matrix)
library(rgdal)
library(ggplot2)
library(readstata13)
library(foreign)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

# Networks 1
Obi Thompson Sargoni

## Learning Objectives

What are the main take aways from this session?

* Defining graphs: nodes, edges and the adjacency matrix.
* Directed vs undirected graphs
* Connected graphs
* Calculating node degree
* Shortest paths and diameter
* Performing the above calculations on the London Tube network

***

## Simple Example

"A network is, in its simlpest form, a collection of points joined together in pairs by lines"
M.E.J Newman, Networks, An Introduction

There are many systems that can be usefully thought of as networks. For example:

* The internet (computers joined together by data connections)
* Food webs (animals and plants joined by preditor-prey relationships)
* Transport networks (intersections joined by roads or stations joined by trains)
* Economic networks (businesses joined by transactions)

***

Consider hypothetical Twitter data. Each column representing who that person follows.

```{r}

dfTwitter <- data.frame("Bonnie" = c(0,0,0,1,1), 
                        "Matt" = c(1,0,0,1,0),
                        "Elsa" = c(0,1,0,0,1),
                        "Neave" = c(1,0,0,0,0),
                        "Obi" = c(0,1,1,0,0),
                        row.names = c("Bonnie", "Matt", "Elsa", "Neave", "Obi"))

dfTwitter
```

***

Now create an iGraph graph object from this data
```{r}
graphTwitter <- graph_from_adjacency_matrix(as.matrix(dfTwitter))
plot(graphTwitter)

```

***

**Graph**

A Graph is a mathematical representation of a network, with n nodes and m edges, as seen above. In this example, the nodes represent Twitter accounts and the edges represent following relationships between the accounts.

Nodes (Vertices)
```{r}
V(graphTwitter)
```

Edges
```{r}
E(graphTwitter)
```

Edge List

An equivalent representation of this network is the edge list, with edges pointing from the nodes in columns 1 to the nodes in column 2.
```{r}
get.edgelist(graphTwitter)
```

***

**Adjacency matrix**

Another important representation of a network is the adjacency matrix.

In iGraph the convention for adjacency matrix elements is (some textbooks use the reverse convention)

$$A_{ij} = \begin{cases} 1, & \text{if there is an edge from node i to j}\\
      0, & \text{otherwise}
      \end{cases}$$

In this case, the dataframe used to create the network is essentially the adjacency matrix:
```{r}
get.adjacency(graphTwitter)
```

$A_{Bonnie,Matt} = 1$

```{r}
graphTwitter['Bonnie','Matt'] # as.matrix(get.adjacency(graphTwitter))['Bonnie','Matt'] also works 
```

***

**Directed Graph**

A directed graph is a graph where an edge points from one node to another.

Directed graphs have asymmetric adjacency matrices.
```{r}
isSymmetric(get.adjacency(graphTwitter))
```

Bonnie is connected to Matt but Matt is not connected to Bonnie
$A_{Bonnie,Matt} \neq A_{Matt, Bonnie}$
```{r}
print(graphTwitter['Bonnie','Matt'])
print(graphTwitter['Matt','Bonnie'])
```

```{r}
plot(graphTwitter)
```


***

## Simple Analysis

With the information about our social network represented as a Graph, we can perform some analysis.

**Node degree**

Undirected graph: the number of links attached to a node
$k_i = \sum_{i=1}^{n}A_{ij} = \sum_{j=1}^{n}A_{ij}$

Directed graph: either the number of 'in' links or 'out' links attached to a node.

Recall:
$$A_{ij} = \begin{cases} 1, & \text{if there is an edge from node i to j}\\
      0, & \text{otherwise}
      \end{cases}$$

In degree: number of links pointing to you
$k_{j}^{in} = \sum_{i=1}^{n}A_{ij}$

Out degree: number of links you point to
$k_{i}^{out} = \sum_{j=1}^{n}A_{ij}$

In degree of the Twitter Graph
```{r}
degree(graphTwitter, mode = "in")
```

Out degree
```{r}
degree(graphTwitter, mode = "out")
```

***

**Paths**

A path is a sequence of nodes such that each node is connected to the next node. For example: ['Bonnie', 'Neave', 'Matt', 'Elsa'].

The shortest path is the shortest possible path between any two nodes.

['Bonnie', 'Neave', 'Matt', 'Elsa'] is a path from Bonnie to Elsa but the shortest path is...
```{r}
shortest_paths(graphTwitter, 'Bonnie', to = 'Elsa', 
               mode = "out", weights = NULL, output = c("vpath", "epath", "both"),
               predecessors = FALSE, inbound.edges = FALSE)
```

or use mode="in" to find the shortest path from the 'to' node to the 'from' node
```{r}
shortest_paths(graphTwitter, 'Bonnie', to = 'Elsa', 
               mode = "in", weights = NULL, output = "vpath",
               predecessors = FALSE, inbound.edges = FALSE)
```

***

**Connected**

A Graph is connected if it is possible to reach any node from any other node. Directed graphs can be 'strongly' or 'weakly' connected.

```{r}
dfDisconnected <- data.frame("Bonnie" = c(0,1,0,0,0), 
                        "Matt" = c(1,0,0,0,0),
                        "Elsa" = c(0,0,0,1,1),
                        "Neave" = c(0,0,0,0,1),
                        "Obi" = c(0,0,1,0,0),
                        row.names = c("Bonnie", "Matt", "Elsa", "Neave", "Obi"))

graphDisconnected <- graph_from_adjacency_matrix(as.matrix(dfDisconnected))
plot(graphDisconnected)
```

```{r}
is.connected(graphDisconnected)
```


* Strongly connected: it is possible to reach every node from every other node whilst following the directions of the edges
* Weakly connected: it is possible to reach every node only if the directions of the edges are disregarded (treat as undirected graph)


The Twitter graph is strongly connected (which means it is also weakly connected)
```{r}
is.connected(graphTwitter, mode = "strong")
```

***

**Diameter**

The diameter of the network is the length of the longest shortest path between nodes for which a path exists. For an disconnected graph the largest connected component diameter is given.

This avoids infinite diameters
```{r}
diameter(graphTwitter, directed = TRUE, unconnected = TRUE, weights = NULL)
```


***

## Tube Network Example

Use GIS data of the London Underground to produce graph representing the tube network.

Stations -> Nodes
Connections between stations -> Edges

**Read the shapefile data**

Linestrings with each end being the coordinates of a station
```{r}
dir_data <- ".\\data\\underground"
sfTube <- readOGR(dsn = dir_data, "underground")
```

The data contain line strings between the coordinates of tube stations
```{r}
head(sfTube@lines)
```

and attributes of the linestrings
```{r}
head(sfTube@data)
```

***

**Create a graph from the data**

Use the attribute data from the shape file to create a graph representing the tube network.

```{r}
dfTube <-  sfTube@data
dfTubeGraph <- data.frame(i = dfTube$station_1, j = dfTube$station_2, distance = dfTube$distance)
graphTube <- graph_from_data_frame(dfTubeGraph, directed = FALSE)
head(get.edgelist(graphTube))
```


**Add node attribute to the graph**

Process the data to get the coordinates, names, and ids of all stations


```{r}
# fortify extracts x and y coordinates from the points that make up the linestrings and the id of the linestring the points belong to
sfTube.f=fortify(sfTube)

# Create id column that matches the ids of the linestrings
dfTube$id <- as.numeric(as.character(dfTube$toid_seq)) - 1

# Join the coordinates to the data from the shape file using the id column
dfMergedTube <- merge(sfTube.f, dfTube, by.x = "id", by.y = "id")

# Separate out the first and second stations and stack these on top of one another in one long dataframe.
firstStation <- dfMergedTube[dfMergedTube$order == 1,]
secondStation <- dfMergedTube[dfMergedTube$order == 2,]

firstStation <- data.frame(id=firstStation$station_1, name = firstStation$station_1_, lat = firstStation$lat, long = firstStation$long)
secondStation <- data.frame(id=secondStation$station_2, name = secondStation$station_2_, lat = secondStation$lat, long = secondStation$long)

# Combine stations, dropping duplicates
dfStations <- rbind(firstStation, secondStation) %>% unique()
head(dfStations)

```

Find the index at which each vertex appears in the dataframe of stations and use this to assign names and positions to the verticies
```{r}
# Since the vertices don't have names yet, V(graphTube)$name returns a vector of vertex ids
v_pos = match(V(graphTube)$name, as.character(dfStations$id))

# Use the vector of positions to get the node names and coordinates ordered the same as they are in the graph
vector_names <- dfStations$name[v_pos]
vector_coords <- dfStations[v_pos,]

graphTube <- set.vertex.attribute(graphTube, "name", index = V(graphTube), value = as.character(vector_names))
graphTube <- set.vertex.attribute(graphTube, "x", index = V(graphTube), value = as.numeric(vector_coords$long))
graphTube <- set.vertex.attribute(graphTube, "y", index = V(graphTube), value = as.numeric(vector_coords$lat))
```


Nodes now all have name, x, and y attributes
```{r}
print(get.vertex.attribute(graphTube, "name", 11))
print(get.vertex.attribute(graphTube, "x", 11))
print(get.vertex.attribute(graphTube, "y", 11))
```

***

## Tube Network Analysis


This graph is undirected and connected.
```{r}
print(isSymmetric(get.adjacency(graphTube)))
print(is.connected(graphTube))
```


The graph includes an edge attribute for each edge - 'distance' - taken from the distance between stations
```{r}
get.edge.attribute(graphTube)$distance[1:10]
```


Show adjacency matrix with and without weights (topographic vs weighted)

For cases where there are multiple edges between nodes, the inclusion of weights can be misleading.
```{r}
get.adjacency(graphTube)[1:10,1:10]
```

```{r}
get.adjacency(graphTube, attr = "distance")[1:10,1:10]
```

From this we see there are two connections between station 87 and 49. Check this in the data.
```{r}
dfTube[dfTube$station_1 == 49,]
```

***

**Degree distribution**

Calculate degree distribution of the tube network
```{r}
hist(degree(graphTube))
```

**Shortest paths and diameter**

A connected graph is one in which every node can be reached by every other node.

Calculate the shortest path between nodes (crude since the weights are distances and not travel times)
```{r}
startNode = V(graphTube)["Stockwell"]
endNode = V(graphTube)["Mile End"]
shortest_paths(graphTube, startNode, to = endNode, weights = get.edge.attribute(graphTube)$distance, output = "vpath")
```

Calculate the diameter with and without using the distance as the weight.
```{r}
print(diameter(graphTube, directed = FALSE, unconnected = FALSE, weights = NULL))
print(diameter(graphTube, directed = FALSE, unconnected = FALSE, weights = get.edge.attribute(graphTube)$distance))
```


Plot the graph
```{r}
graphTube=simplify(graphTube,remove.loops = T,remove.multiple = T,edge.attr.comb = "min")
plot(graphTube,vertex.size=3,vertex.color="red",vertex.label.cex=.1)
```

***

## Further Reading

Resources from a workshop on networks and iGraph: https://kateto.net/wp-content/uploads/2016/01/NetSciX_2016_Workshop.pdf

M.E.J Newman, Networks, An Introductions

***

# Networks 2 
Elsa Arcaute

## Learning Objectives

* Introduce three centrality measures: degree, closeness and betweeness centrality
* Calculate these centrality measures for a social network
* Introduce Assortativity and modularity
* Introduce hierarchical clustering for community detection
* Apply hierarchical clustering to the karate club dataset


## Centrality

Nodes within a netork can play different roles, and have different levels of importance. Centrality measures help determine what these roles are. In this sense, there is no unique measure of importance, since this is determined with respect to a specific situation/problem at hand. For example, a person might be very important in leading a group of people, while a different one in mediating conflict and so forth. Some examples of centrality measures are:

* Degree centrality
* Eigenvector centrality
* Katz Centrality
* Closeness centrality
* Betweeness centrality

Within this section we will introduce degree, closeness and betweenness centrality.

***

**Degree centrality**

Degree was introduced in the previous section (1.3). Let us take an example of a social network, and see how it works.

I asked my then 8 years old son Yaan to construct a small immediate network of people important for his dad David. How? build a list of person 1| person 2| weight, where the weight is a number between 1 and 10 relating to the importance of the relationship, 10 being the highest.

```{r}
#file constructed by Yaan
file_network <- ".\\data\\network_David.csv"

#read and get network
dat=read.csv(file_network,header=TRUE)
head(dat)
g_david=graph.data.frame(dat,directed=FALSE)

#let us plot it
plot(g_david)

#Plotting the network in this way does not give us enough information
#Now let us compute the degree centrality
deg_david=degree(g_david, v=V(g_david), loops = F, normalized = FALSE)

print(deg_david)

#we can plot the graph using the measure of degree
plot(g_david,vertex.size=deg_david/max(deg_david)*14,vertex.color=deg_david,vertex.label.cex=0.5)
title(main = "Social network of David seen by Yaan")

```

In this plot we can see who is the most connected person, any surprises?

Now recall the previous section in which you learnt that there's 'in' and 'out' degree. Who is more important?

What if instead of looking at the *number* of connections you have, you look at how important those connections are? -->  **Eigenvector centrality** 
More elaborated algorithms looking at not only the importance of the connections, but at the connections pointing *to you*, etc, are the ones used to rank internet pages for example, such as **Page Rank**.


**Closeness centrality**

Let $d_{ij}$ be the geodesic distance (shortest path) between node $i$ and $j$. The mean geodesic distance of node $i$ to all other nodes is given by:
$$l_{i} = \frac{1}{n}\sum_{j}d_{ij}$$
where $n$ is the total number of nodes.

Given that a node that is very close to most nodes, and has hence low mean geodesic, will be more influential than a node which is far away, gives rise to the following definition for the closeness centrality of node $i$:
$$C_{i} = \frac{1}{l_{i}} = \frac{n}{\sum_{j}d_{ij}}$$


Let us compute the closeness centrality for each node in David's network. Note that when using a software, it is important that you always read the documentation, since for example, in the case of iGraph in R, if not specified, the measure is not divided by the total n. of nodes. If normalised it will be normalised by $n-1$.

The link to the documentation is here:
https://igraph.org/r/doc/closeness.html


```{r}
# given that the network is weighted let us introduce the weights.
v_weights=1/(E(g_david)$weight)

# Note that we inverted the weights, since they are meant to represent distance.
# the higher the value to closer they are
clos_david=closeness(g_david,weights = v_weights)

# the following two commands are just to choose colours for the nodes, a palette
normalised_clos_david=(clos_david-min(clos_david))/(max(clos_david)-min(clos_david))
palette=hsv(h=1-((normalised_clos_david*2/3)+1/3),s = 1,v=1)

# Not we can plot it
plot(g_david,vertex.color=palette,vertex.size=normalised_clos_david*15,vertex.label.cex=.5)
title(main = "Closeness centrality of David's network according to Yaan")
```

And now who is the most important person in the network according to closeness centrality? Is this surprising?

The measure computed above took into account the weights of the network. This means that the distance between two nodes that are not connected, might be shorter than the distance between two nodes that are connected but have very low "friendship" number. If this is disregarded, the measure only corresponds to the shortest path computed according to the number of links between them.  

```{r}
#topological closeness
clos_top_david=closeness(g_david, weights = NA)
normalised_clos_top_david=(clos_top_david-min(clos_top_david))/(max(clos_top_david)-min(clos_top_david))
palette_top=hsv(h=1-((normalised_clos_top_david*2/3)+1/3),s = 1,v=1)
plot(g_david,vertex.color=palette_top,vertex.size=normalised_clos_top_david*15,vertex.label.cex=.5)
title(main = "Topological closeness centrality")

```


**Betweenness centrality**

The betweenness centrality of a vertex corresponds to the number of shortest paths passing through it among all pairs. Edge betweennness is defined in a smilar, where the edge is within the shortest path.

Define $n^{i}_{st}$ as:
$$n^{i}_{st} = \begin{cases} 1, & \text{if vertex }i\text{ lies on the geodesic path from }s\text{ to }t\\
      0, & \text{otherwise}
      \end{cases}$$

Then betweenness centrality can be defined as:
$$x_{i} = \sum_{st}n^{i}_{st}$$

However, there may be multiple geodesics from $s$ to $t$ so to account for this we normalise by the number of geodesics from $s$ to $t$, $g_{st}$:
$$x_{i} = \sum_{st}\frac{n^{i}_{st}}{g_{st}}$$

```{r}
#Let us compute the betweenness centrality for the network 
bet_david=betweenness(g_david, v=V(g_david), directed = F, normalized = FALSE, weights = (E(g_david)$weight))
palette=hsv(h=1-((bet_david/max(bet_david)*2/3)+1/3),s = 1,v=1)
plot(g_david,vertex.color=palette,vertex.size=bet_david/max(bet_david)*18,vertex.label.cex=.5)
title(main = "Broker of David's network seen by Yaan")

```

This is a rather nice result: the grandma is the broker of the system!

Note that such a nice result fooled us. Why? What needs to be ammended?

Let us look at the topological betweenness and see whether we get any insights:

```{r}
#Topological betweenness centrality for the network 
bet_david_top=betweenness(g_david, v=V(g_david), directed = F, normalized = FALSE, weights = NA)
palette_top=hsv(h=1-((bet_david_top/max(bet_david_top)*2/3)+1/3),s = 1,v=1)
plot(g_david,vertex.color=palette_top,vertex.size=bet_david_top/max(bet_david_top)*18,vertex.label.cex=.5)
title(main = "Broker (topological) of David's network seen by Yaan")

```

There are 2 lessons to learn from this:

* assigning arbitrary numbers to relationships can lead to misleading results

* we need to be careful when using weights: what are they encoding? how are they representing the system? how can we integrate them correctly?


## Assortativity and Modularity 

In order to talk about clustering in networks, one needs to introduce a set of concepts, such as clustering coefficient, modularity, assortativity, etc., which need more than the few remaining minutes that we have. In this sense, we will give you here "a taster" of this field, since community detection is a field in its own right within networks.

**Assortativity, dissassortativity **
```{r}
include_graphics(".\\img\\networks2\\assort.png")
```


```{r}
include_graphics(".\\img\\networks2\\assort2.png")
```


Let us look at another example where we do not have more information on the ndoes than what you see below. 
```{r}
include_graphics(".\\img\\networks2\\assort_2nets_plus.png")
```


```{r}
include_graphics(".\\img\\networks2\\assort_null.png")
```


```{r}
include_graphics(".\\img\\networks2\\modularity.png")
```



```{r}
include_graphics(".\\img\\networks2\\comm_detection.png")
```



## Hierarchical clustering

There are two types of hierarchical clustering algorithm: **Agglomerative** and *Divisive*

--> both methods are based on assigning a measure for each link, say a similarity measure between nodes, or a centrlaity measure for the link, and then depending on the algorithm we have: either nodes connected by high value links (e.g. high similarity) are merged, or links with low value are removed (e.g. for low similarity).

The process for hierarchical clustering is as follows:

1. Define a similarity measure, or a centrality measure

2. Commpute the similarity or centrality measure for the network

3. Iteratively merge nodes or remove links until all nodes are merged, or all links are removed

4. Build a dendogram odf the process

4. Identify the cut in the tree that maximises the modularity



The following slides taken from Barabasi's lecture for the Girvan-Newman algorithm illustrate the method very clearly

```{r}
include_graphics(".\\img\\networks2\\G_N1.png")
```



```{r}
include_graphics(".\\img\\networks2\\G_N2.png")
```



**Karate Club network**

In this section we apply some of the community detection algorithms available in iGraph to Zachary's Karate club network. Look it up, this is the most famous netwrk to test community detection algorithms.

```{r}
#It's so famous it's already in iGraph
g_karate <- make_graph("Zachary")
#let us now produce a plot such that the nodes in the plot have a size related to their degree
V(g_karate)$size=degree(g_karate)
plot(g_karate,vertex.label=NA)
title('Karate club friendship network', cex.main = 1.25,line=2)
title('Zachary 1977', cex.main = 1, line=1)
```



**Girvan Newman method**
M. Girvan and M. E. Newman, Proc. Natl. Acad. Sci. U.S.A., 99, 782 (2002)

M. E. Newman and M. Girvan, Physical Review E 69, 026113 (2004)

iGraph: edge.betweenness.community

http://igraph.org/r/doc/community.edge.betweenness.html

http://www.inside-r.org/packages/cran/igraph/docs/edge.betweenness.community

*How does it work?*

* Hierarchical divisive algorithm: remove edges (links) according to their edge betweenness values (in decreasing order).

* Removal stops when modularity of the partition is maximised.

--> Edges with high betweenness will most likely be the ones connecting and hence belonging to different groups.

*Performance*

* Good

* slow: O(N3): betweenness needs to be re-calculated each time that an edge is removed

**Weighted graphs?** Yes

**Directed graphs?** Yes

```{r}
#------------
#Girvan-Newman algorithm
#------------
	alg_name='Girvan-Newman'
	GN_comm <- edge.betweenness.community(g_karate)
	#look at result of algorithm
	print(GN_comm)
	#the process aims at maximising the modularity
	modularity(GN_comm)
	# number of communities and their sizes
	nc=length(GN_comm)
	sizes(GN_comm)
	
	#if you need the membership vector only
	mem_vec=membership(GN_comm)
	#visualise the results
	plot(GN_comm,g_karate,layout=layout.fruchterman.reingold)
	title(paste0(alg_name,' algorithm \n',nc,' communities for the karate club'))
	#The division into communities is determined by the choice of location for cutting  the hierarchical tree.
	#The threshold is selected such that the modularity is maximised
	#Let us look at this through the dendrogram
	dendPlot(GN_comm,use.modularity = TRUE)
	title(paste0('Community structure dendrogram for ',alg_name,' method \n',nc,' communities for the karate club'))
# !!!!!! This method is based on computing betweenness in an iterative way, and hence it is extremely slow for large networks. 

```
**Fast Greedy modularity optimization: Clauset, Newman and Moore**

Clauset, M. E. J. Newman, and C. Moore, Phys. Rev. E 70, 066111 (2004)

http://www.arxiv.org/abs/cond-mat/0408187

iGraph: fastgreedy.community

http://igraph.org/r/doc/fastgreedy.community.html

http://www.inside-r.org/packages/cran/igraph/docs/fastgreedy.community

*How does it work?*

* Assign each node to its own community, no links between communities

* Inspect each community pair connected by a link, and assign them to the same community if the link increases maximally the Girvan-Newman modularity.

* Bottom-up instead of top-down: communities are merged iteratively such that each merge is locally optimal

*Performance*

* Not as good as Girvan-Newman

* fast: O(NLog2N)

* Resolution limit

**Weighted graphs?** Yes

**Directed graphs?** No

```{r}
#------------
#Fast greedy modularity optimisation: Clauset-Newman-Moore
#------------
	alg_name='Clauset-Newman-Moore'
	greedy_c <- fastgreedy.community(g_karate)
	#look at result of algorithm
	print(greedy_c)
	#the process aims at maximising the modularity
	modularity(greedy_c)
	# number of communities and their sizes
	nc=length(greedy_c)
	sizes(greedy_c)
	
	#if you need the membership vector only
	mem_vec=membership(greedy_c)
	#visualise the results
	plot(greedy_c,g_karate)#,layout=layout.fruchterman.reingold)
	title(paste0(alg_name,' algorithm \n',nc,' communities for the karate club'))
	#The division into communities is determined by the choice of location for cutting  the hierarchical tree.
	#The threshold is selected such that the modularity is maximised
	#Let us look at this through the dendrogram
	dendPlot(greedy_c,use.modularity = TRUE)
	title(paste0('Community structure dendrogram for ',alg_name,' method \n',nc,' communities for the karate club'))

```
**Random walk: Pons & Latapy**

Pascal Pons, Matthieu Latapy: Computing communities in large networks using random walks, http://arxiv.org/abs/physics/0512106

iGraph: walktrap.community

*How does it work?*

* This function tries to find densely connected subgraphs

* short random walks tend to stay in the same community

* merge separate communities in a bottom-up manner
 * can use the modularity score to select where to cut the dendrogram

*Performance*

* Slower than greedy
* More accurate than greedy


**Weighted graphs?** Yes

**Directed graphs?** No
```{r}
#------------
#Community strucure via short random walks: Pons&Latapy
#------------
	alg_name='Random walks'
	wtrap_c <- walktrap.community(g_karate)
	comm=wtrap_c
	#look at result of algorithm
	print(comm)
	#the process aims at maximising the modularity
	modularity(comm)
	# number of communities and their sizes
	nc=length(comm)
	sizes(comm)
	
	#if you need the membership vector only
	mem_vec=membership(comm)
	#visualise the results
	plot(comm,g_karate)#,layout=layout.fruchterman.reingold)
	title(paste0(alg_name,' algorithm \n',nc,' communities for the karate club'))
	#The division into communities is determined by the choice of location for cutting  the hierarchical tree.
	#The threshold is selected such that the modularity is maximised
	#Let us look at this through the dendrogram
	dendPlot(comm,use.modularity = TRUE)
	title(paste0('Community structure dendrogram for ',alg_name,' method \n',nc,' communities for the karate club'))

```
**Leading eigenvector: Newman spectral approach**

MEJ Newman: Finding community structure using the eigenvectors of matrices, Physical Review E 74 036104, (2006)

iGraph: leading.eigenvector.community

http://igraph.org/r/doc/leading.eigenvector.html

http://www.inside-r.org/packages/cran/igraph/docs/leading.eigenvector.community

*How does it work?*

* Method based on the spectral properties of graphs: IF communities well identified, e-vector components for nodes in same communities will have similar values

* compute eigenvector of modularity matrix for the largest positive eigenvalue.

* separate vertices into two community based on the sign of the corresponding element in the eigenvector. If all elements in the eigenvector are of the same sign that means that the network has no underlying community structure.

* Choose partition that maximises the G-N modularity

*Performance*

* Fast if select only a few e-vectors to compute, although slower than greedy

* Good results

**Weighted graphs?** No

**Directed graphs?** No


```{r}
#------------
#Leading eigenvector: Newman spectral approach
#------------
	alg_name='Spectral'
	spectral_c <- leading.eigenvector.community(g_karate)
	comm=spectral_c
	#look at result of algorithm
	print(comm)
	#the process aims at maximising the modularity
	modularity(comm)
	# number of communities and their sizes
	nc=length(comm)
	sizes(comm)
	
	#if you need the membership vector only
	mem_vec=membership(comm)
	#visualise the results
	plot(comm,g_karate)#,layout=layout.fruchterman.reingold)
	title(paste0(alg_name,' algorithm \n',nc,' communities for the karate club'))
	#Doesn't get a dendrogram

```
**Louvain: Blondel et al, modularity optimization**

V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, J. Stat. Mech.: Theory Exp. 2008, P10008.

http://arxiv.org/abs/arXiv:0803.0476

iGraph:  multilevel.community

http://igraph.org/r/doc/multilevel.community.html

http://www.inside-r.org/packages/cran/igraph/docs/multilevel.community


*How does it work?*

* Local optimization of the G-N modularity in the neighbourhood of each node

* Start from the whole network, identify a community, replace the community by a super-node --> get smaller weighted and repeat

* Iterate until modularity stable

*Performance*

* Excellent 

* Computational linear complexity w.r.t. n. nodes

**Weighted graphs?** Yes

**Directed graphs?** No

```{r}
#------------
#Louvain method: Blondel et al, modularity optimization
#------------
	alg_name='Louvain'
	louv_c <- multilevel.community(g_karate)
	comm=louv_c
	#look at result of algorithm
	print(comm)
	#the process aims at maximising the modularity
	modularity(comm)
	# number of communities and their sizes
	nc=length(comm)
	sizes(comm)
	
	#if you need the membership vector only
	mem_vec=membership(comm)
	#visualise the results
	plot(comm,g_karate)#,layout=layout.fruchterman.reingold)
	title(paste0(alg_name,' algorithm \n',nc,' communities for the karate club'))

```
**Infomap: Rosvall & Bergstrom**

M. Rosvall and C. T. Bergstrom, Maps of information flow reveal community structure in complex networks, PNAS 105, 1118 (2008) 

M. Rosvall, D. Axelsson, and C. T. Bergstrom, The map equation, Eur. Phys. J. Special Topics 178, 13 (2009). 

iGraph: infomap.community

http://igraph.org/r/doc/infomap.html

http://www.inside-r.org/packages/cran/igraph/docs/infomap.community

*How does it work?*

Find community structure that minimizes the expected description length of a random walker trajectory

*Performance*

* Fast
* Excellent, best performance

**Weighted graphs?** Yes

**Directed graphs?** Yes

```{r}
#------------
#Infomap method: Rosvall and Bergstrom
#------------
	alg_name='Infomap'
	info_c <- infomap.community(g_karate)
	comm=info_c
	#look at result of algorithm
	print(comm)
	#the process aims at maximising the modularity
	modularity(comm)
	# number of communities and their sizes
	nc=length(comm)
	sizes(comm)
	
	#if you need the membership vector only
	mem_vec=membership(comm)
	#visualise the results
	plot(comm,g_karate)#,layout=layout.fruchterman.reingold)
	title(paste0(alg_name,' algorithm \n',nc,' communities for the karate club'))

```

```{r}
include_graphics(".\\img\\networks2\\table_mod.png")
```

```{r}
include_graphics(".\\img\\networks2\\Expert.png")
```

```{r}
include_graphics(".\\img\\networks2\\Expert2.png")
```


# Networks 3 Neave



```{r echo=FALSE}
include_graphics(".\\img\\UCL-Economic-Complexity-Intro.png")
```


## Learning Objectives

This session will cover the following:

* How trade data can be used to model the capabilities of countries
* Calculating Relative Compatative Advantage (RCA) for a country and product
* Introduce The Product Space and using RCA to compute the proximity between products
* Identifying products a country has the capacity to succeed at exporting

The code presented here follows the methods used in *The Product Space Conditions the Development of Nations* By C. A. Hidalgo, B. Klinger, A.-L. BarabÃ¡si, R. Hausmann, Science Jul 2007

It's a great paper and worth a read.

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Literature.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Measure-Economic-Complexity.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Scrabble-Analogy.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Economic-Compexity-Def.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Trade-Data.png")
```

***

## Trade Data

You can access world import and export data from the United Nations here: https://cid.econ.ucdavis.edu/nberus.html.

See page 48 of the documentation (.\\data\\wtf99\\NBER-UN_Data_Documentation_w11040.pdf) for an explanation of the variables.

```{r}
# Load world import and export data from 1999. We restrict our analysis to a single year for ease but it is better to use all available years.
dfWorldTrade <- read.dta13(".\\data\\wtf99\\wtf99.dta")
head(dfWorldTrade)
```

Products are differentiated by their sitc4 code

```{r}
# 1263 different product codes includes 1 blank code so 1262 in practice
length(unique(dfWorldTrade$sitc4))
```
```{r}
# 188 unique exporter countries
length(unique(dfWorldTrade$ecode))
```

Data cleaning
```{r}
# Remove invalid product codes and select only the columns we are interested in
dfWorldTrade <- dfWorldTrade[dfWorldTrade$sitc4 !="",c("ecode","sitc4","value")]

# Remove entries where the exporter is 'World'. The World category is assigned to imports/exports with missing data.
dfWorldTrade <- dfWorldTrade[dfWorldTrade$ecode != "100000",]

head(dfWorldTrade)
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-RCA.png")
```

***

## Revealed Comparative Advantage

Group by country and product and calculate the value of each county's exports by product
```{r}
# Aggregate the data
dfWTCountryAgg <- aggregate(dfWorldTrade$value, by = list(dfWorldTrade$ecode), sum, na.rm = TRUE)
dfWTCountryProdAgg <- aggregate(dfWorldTrade$value, by = list(dfWorldTrade$ecode, dfWorldTrade$sitc4), sum, na.rm = TRUE)
dfWTProductAgg <- aggregate(dfWorldTrade$value, by = list(dfWorldTrade$sitc4), sum, na.rm = TRUE)
globalSum <- sum(dfWorldTrade$value)

# Rename the columns
colnames(dfWTCountryAgg) <- c("ecode", "etotal")
colnames(dfWTProductAgg) <- c("sitc4", "ptotal")
colnames(dfWTCountryProdAgg) <- c("ecode", "sitc4", "eptotal")
```
```{r}
head(dfWTCountryAgg)
```
```{r}
head(dfWTProductAgg)
```


```{r}
head(dfWTCountryProdAgg)
```

Join these datasets together and use this to calculate RCA
```{r}
dfRCA <- merge(dfWTCountryProdAgg, dfWTCountryAgg, by ="ecode")
dfRCA <- merge(dfRCA, dfWTProductAgg, by="sitc4")
dfRCA$prod_in_country_share <-  dfRCA$eptotal / dfRCA$etotal
dfRCA$prod_in_global_share <- dfRCA$ptotal / globalSum
dfRCA$rca = dfRCA$prod_in_country_share / dfRCA$prod_in_global_share
head(dfRCA)
```

RCA > 1 means that this product makes up a greater share of this country's exports than the "average" country.

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Relatedness.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Path-dependence.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-The-Product-Space.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Exports.png")
```

***

## The Product Space

To calculate the proximity between products we use RCA as an indication of whether a country exports a product or not. If a country has an RCA > 1 for a product it is considered to export that product.

Using this the coditional probability of a product being exported given another product being exported is given by:

$$P(product_{i} | product_{j}) = \frac{\text{number of countries exporting i and j}}{\text{number of countries exporting j}}$$

```{r}
# Number of counties exporting each product
dfRCA$is_exported <- as.numeric(dfRCA$rca > 1)
dfNExport <- aggregate(dfRCA$is_exported, by = list(dfRCA$sitc4), sum)
colnames(dfNExport)[2] <- "nexporters"
head(dfNExport)
```


```{r}
# Number of countries exporting product paris
# This block takes a long time to run. To save time (and avoid crashing your computer), you can read the data from csv using the next block instead.

# Merge on country code to match exports to one another if they are both exported by the same country, then count the number of countries than export both products
#dfPairs <- merge(dfRCA, dfRCA, by = "ecode")
#dfPairs$joint_export <-  as.numeric((dfPairs$is_exported.x == 1) & (dfPairs$is_exported.y ==1))
#dfNJointExport <- aggregate(dfPairs$joint_export, by = list(dfPairs$sitc4.x,dfPairs$sitc4.y), sum)
#colnames(dfNJointExport)[3] <- "njointexporters"
#write.csv(dfNJointExport, ".\\data\\product_joint_export_counts.csv", row.names=FALSE)
#head(dfNJointExport)
```

```{r}
dfNJointExport <- read.csv(".\\data\\product_joint_export_counts.csv")
head(dfNJointExport)
```


Use these two tables to compute the conditional probabilities
```{r}
dfConditional <- merge(dfNJointExport, dfNExport, by = 'Group.1')
dfConditional$pij <- dfConditional$njointexporters / dfConditional$nexporters
head(dfConditional)
```


Now identify and select the minimum joint probability by joining the data frame of confitional probabilities to itself.
```{r}
# Merge the dfConditional dataframe to itself so that we can compare the joint probabilities P(export i | export j) to P(export j | export i)
dfProximity <- merge(dfConditional, dfConditional, by.x = c("Group.1","Group.2"), by.y = c("Group.2", "Group.1"))
dfProximity$pij.x.islower <- dfProximity$pij.x < dfProximity$pij.y
head(dfProximity)
```


```{r}
# We can check this merge has given us the right thing by checking the conditional probability value for Group.1 = "0015" and Group.2 = "0011" has been merged with the value for Group.1 = "0011" and Group.2 = "0015"
dfConditional[(dfConditional$Group.1=="0015") & (dfConditional$Group.2 == "0011"),]
```

```{r}
# Now select the lower conditional probabilities. The lower conditional probability is used as the proximity between two products
dfProximity1 <- dfProximity[dfProximity$pij.x.islower ==TRUE, c("Group.1", "Group.2", "pij.x")]
dfProximity2 <- dfProximity[dfProximity$pij.x.islower ==FALSE, c("Group.1", "Group.2", "pij.y")]

colnames(dfProximity1) <- c("product1","product2", "proximity")
colnames(dfProximity2) <- c("product1","product2", "proximity")

dfProximity <- rbind(dfProximity1, dfProximity2)

# Drop entries where product1 == product2 as the proximity is trivally 1
dfProximity <- dfProximity[dfProximity$product1!=dfProximity$product2,]

# Check for duplicates
any(duplicated(dfProximity[,1:2]))
```


This dataframe can be used to build a network where the nodes are the products and the edge weights between products are the proximity.
```{r}
head(dfProximity)
```

Create an iGraph object from this edge list
```{r}
graphProdSpace <- graph_from_data_frame(dfProximity, directed = FALSE)
```

This is a very dense graph so plotting the whole thing will not be helpful. In the Hidalgo (2007) paper the authors describe the variety of methods they used to represent the network visually and produce a figure like that in the slide below. See the supporting online material for details: https://science.sciencemag.org/content/sci/suppl/2007/07/26/317.5837.482.DC1/Hidalgo.SOM.pdf

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-The-Product-Space-Figure.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Irelands-Exports.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Diversification.png")
```

***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Diversification-in-Product-Space.png")
```

## Diversification

Calculate 'density' around products for a single country (eg UK)
```{r}
# First subsect the product space to consider only the products exported by a particular country, eg the UK

# The data documentation (.\\data\\wtf99\\NBER-UN_Data_Documentation_w11040.pdf) contains a lookup from country name to country code. The UK is 538260

# Get all products extored by the UK
ukExports <- dfRCA[(dfRCA$is_exported == 1) & (dfRCA$ecode == "538260"),"sitc4"]

# Using dfProximity as the edge list, remove connections to nodes not exported by the UK
dfProximityUK <- dfProximity[dfProximity$product2 %in% ukExports,]

# Now aggregate by product to get the weight contribution from UK exported products only
dfStrengthUK <- aggregate(dfProximityUK$proximity, by = list(dfProximityUK$product1), sum)
colnames(dfStrengthUK) <- c("product","strengthUK")
head(dfStrengthUK)
```


```{r}
# The denominator of the density of a node is given by the sum of its edge weights. This is similar to the node degree but instead we use the edge function
strengthProdSpace <- strength(graphProdSpace,weights = get.edge.attribute(graphProdSpace)$proximity)

dfStrength <- data.frame(product = names(strengthProdSpace), strength = strengthProdSpace)
rownames(dfStrength) <- c(1:length(dfStrength$product))
head(dfStrength)
```


```{r}
# Now combine together and identify the high density products the UK does not export
dfStrength <- merge(dfStrength, dfStrengthUK)
dfStrength$density <- dfStrength$strengthUK / dfStrength$strength

# Now select just the products that the UK doesn't export and rank by strength
dfUKNotExp <- dfStrength[!(dfStrength$product %in% ukExports),]
head(dfUKNotExp[with(dfUKNotExp, order(density, decreasing = TRUE)),])
```


What products are these?
```{r}
# The descriptions for revision 1 and 2 of the SITC codes can be downloaded from here: https://unstats.un.org/unsd/tradekb/Knowledgebase/50262/Commodity-Indexes-for-the-Standard-International-Trade-Classification-Revision-3

# The codes used here are the 4th revision but we will use the descriptions from the 2nd revision for ease.

# The 4th revision is detailed here: https://unstats.un.org/unsd/publications/catalogue?selectID=104
dfSITC <- read.csv(".\\data\\SITC Rev2.csv")
colnames(dfSITC) <- c("commodity.code", "commodity.description")
head(dfSITC)
```

```{r}
# Add product descriptions into the stength dataframe
dfStrength$product_group <- substr(dfStrength$product, start = 1, stop = 2)
dfStrength <- merge(dfStrength, dfSITC, by.x = c("product"), by.y = c("commodity.code"))
dfStrength <- merge(dfStrength, dfSITC, by.x = c("product_group"), by.y = c("commodity.code"))
colnames(dfStrength)[6:7] <- c("product_description","group_description")
head(dfStrength)
```

```{r}
# Now select just the products that the UK doesn't export and rank by strength
dfUKNotExp <- dfStrength[!(dfStrength$product %in% ukExports),]
head(dfUKNotExp[with(dfUKNotExp, order(density, decreasing = TRUE)),])
```

```{r}
# What product *does* the UK import that have the highest density
dfUKExp <- dfStrength[dfStrength$product %in% ukExports,]
head(dfUKExp[with(dfUKExp, order(density, decreasing = TRUE)),])
```


***

```{r echo=FALSE}
include_graphics(".\\img\\UCL-Prediction-using-Density.png")
```